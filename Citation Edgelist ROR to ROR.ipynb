{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4afd59bb",
   "metadata": {},
   "source": [
    "# Citation Edgelist ROR to ROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d2aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from collections import Counter\n",
    "from os import path\n",
    "from pyate import combo_basic\n",
    "from math import log, e\n",
    "import math\n",
    "\n",
    "# N.B., need to install dfply ----\n",
    "from dfply import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7482f39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Functions\n",
    "###############################\n",
    "\n",
    "def normalize_counter(x):\n",
    "\ttotal = sum(x.values(), 0.0)\n",
    "\tfor key in x:\n",
    "\t\tx[key] /= total\n",
    "\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ea901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Input Parameters\n",
    "###############################\n",
    "\n",
    "# Do we include all authors or just the first/last author -------\n",
    "input_author_position = \"AllAuthors\"\n",
    "\n",
    "if input_author_position == \"AuthorFirstLast\":\n",
    "\tremove_position_ = \"middle\"\n",
    "else:\n",
    "\tremove_position_ = \"ANYTHING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbc8b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Read in File\n",
    "###############################\n",
    "\n",
    "being_cited_df = pd.read_csv(\"INPUT_SQL_Citation_Data_Astronomy_and_Astrophysics.csv\")\n",
    "\n",
    "ror_df = pd.read_csv(\"INPUT_ROR_Metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "276eaa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n",
      "/tmp/ipykernel_61538/358318943.py:10: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### Prepare File\n",
    "###############################\n",
    "\n",
    "being_cited_df[\"cited_publication_year\"] = being_cited_df[\"cited_publication_year\"].astype(int)\n",
    "\n",
    "year_list = being_cited_year_list = being_cited_df.cited_publication_year.drop_duplicates().tolist()\n",
    "year_list.sort()\n",
    "\n",
    "being_cited_dict = {year_:being_cited_df.query(\"cited_publication_year==@year_\")[['cited_work_id','cited_ror_paper','citing_paper']].drop_duplicates().set_index('cited_work_id').T.to_dict('list') for year_ in being_cited_year_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9490d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Loop through each year in discipline\n",
    "###############################\n",
    "\n",
    "citation_list = []\n",
    "full_citation_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each year ---------\n",
    "for input_window in [2,5,10]:\n",
    "\t\n",
    "\tupdated_year_list = [year_ for year_ in year_list  if (year_+input_window<(2022-input_window))]\n",
    "\n",
    "\tfor year in year_list:\t\n",
    "\n",
    "\t\t\tbeing_cited_year_ = {article_id: article_ for article_id, article_ in being_cited_dict[year].items()}\n",
    "\n",
    "\t\t\tfor article_id, article_ in being_cited_year_.items():\n",
    "\n",
    "\t\t\t\tif (pd.isna(article_[1])):\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# N.B., Articles need to have received at least two citations ----------\n",
    "\t\t\t\tif (len(article_[1].split(\" \"))>1):\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\treceiver_ = normalize_counter(Counter([x.split('+')[0] for x in article_[0].split(\"@\") if remove_position_ not in x]))\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tsender_ = [x.split('^^')[1:3] for x in article_[1].split(\" \") if \"Different_Field\" not in x and  int(article_[1].split(\"^^\")[2]) <= (year+input_window)]\n",
    "\n",
    "\t\t\t\t\t\tsender_ = sum([normalize_counter(Counter([y.split('+')[0] for y in x[0].split(\"@\") if (remove_position_ not in y)])) for x in sender_ if x!=[]],Counter())\n",
    "\t\t\t\t\t\tcitation_list.append([year,receiver_,sender_])\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tcontinue \n",
    "\n",
    "\t\t\tcitation_df = pd.DataFrame(citation_list,columns=[\"Year\",\"Receiver\",\"Sender\"])\n",
    "\n",
    "\t\t\tcitation_df = pd.DataFrame([(year, receiver_, k,v) for (year,receiver_,sender_) in citation_df.values for k, v in sender_.items()],columns=[\"Year\",\"Receiver\",\"Sender_ROR\",\"Sender_Cite\"])\n",
    "\n",
    "\t\t\tcitation_df = pd.DataFrame([(year,k,v,sender_,Sender_cite) for (year,receiver_,sender_,Sender_cite) in citation_df.values for k, v in receiver_.items()],columns=[\"Year\",\"Receiver_ROR\",\"Receiver_Cite\",\"Sender_ROR\",\"Sender_Cite\"])\n",
    "\n",
    "\t\t\t# N.B., Need 'dfply' -----------\n",
    "\t\t\tcitation_df = (citation_df>>\n",
    "\t\t\t\tgroup_by(X.Year,X.Receiver_ROR,X.Sender_ROR)>>\n",
    "\t\t\t\tsummarize(Citations = (X.Sender_Cite * X.Receiver_Cite).sum()))\n",
    "\n",
    "\t\t\tcitation_df = citation_df.query(\"Receiver_ROR!='' & Sender_ROR!=''\")\n",
    "\t\t\tcitation_df[\"N_Citations_Received_Timeframe\"] = input_window\n",
    "\t\t\tfull_citation_df = pd.concat([full_citation_df, citation_df], ignore_index=True) #full_citation_df = full_citation_df.append(citation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72cb7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Combine with ROR DataFrame\n",
    "###############################\n",
    "\n",
    "full_citation_with_ROR_df = pd.merge(full_citation_df,ror_df[[\"country.country_name\",\"id\",\"name\"]].rename(columns={\"country.country_name\":\"Country_Sender\",\"name\":\"ROR_Sender_Name\",\"id\":\"Sender_ROR\"}),on=[\"Sender_ROR\"],how=\"left\")\n",
    "\n",
    "full_citation_with_ROR_df = pd.merge(full_citation_with_ROR_df,ror_df[[\"country.country_name\",\"id\",\"name\"]].rename(columns={\"country.country_name\":\"Country_Receiver\",\"name\":\"ROR_Receiver_Name\",\"id\":\"Receiver_ROR\"}),on=[\"Receiver_ROR\"],how=\"left\")\n",
    "\n",
    "full_citation_with_ROR_df = full_citation_with_ROR_df.drop(columns=[\"Receiver_ROR\",\"Sender_ROR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b9c6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### Output final file to CSV\n",
    "###############################\n",
    "\n",
    "# N.B., need to update file name --------------\n",
    "full_citation_with_ROR_df.to_csv(\"edge_list_ROR_to_ROR.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85946929",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'full_citation_with_ROR_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfull_citation_with_ROR_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'full_citation_with_ROR_df' is not defined"
     ]
    }
   ],
   "source": [
    "full_citation_with_ROR_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ddda59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: networkx in /home/u22/harsh24/.local/lib/python3.8/site-packages (3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17c5e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting node2vec\n",
      "  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: gensim<5.0.0,>=4.1.2 in /home/u22/harsh24/.local/lib/python3.8/site-packages (from node2vec) (4.3.2)\n",
      "Requirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/u22/harsh24/.local/lib/python3.8/site-packages (from node2vec) (1.3.2)\n",
      "Collecting networkx<3.0,>=2.5 (from node2vec)\n",
      "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/u22/harsh24/.local/lib/python3.8/site-packages (from node2vec) (1.24.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.55.1 in /home/u22/harsh24/.local/lib/python3.8/site-packages (from node2vec) (4.66.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/u22/harsh24/.local/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/u22/harsh24/.local/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (6.4.0)\n",
      "Installing collected packages: networkx, node2vec\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.0.1 requires triton==2.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed networkx-2.8.8 node2vec-0.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install node2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016a19d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
