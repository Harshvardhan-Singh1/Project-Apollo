---
title: "Time Interval wise STM Model"
author: "Charles J. Gomez, Harshvardhan Singh"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
header-includes: \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Titles Analysis

```{r cache=TRUE}

library(stringr)
library(ggplot2)
library(wordcloud)
library(stm)

#Read csv file
data = read.csv("preprocessed_data_Jul14.csv")

# Define the publication intervals
pub_intervals <- c("pub_interval_2020_2022", "pub_interval_2015_2019", "pub_interval_2010_2014", 
                   "pub_interval_2005_2009", "pub_interval_2000_2004", "pub_interval_1995_1999",
                   "pub_interval_1985_1994", "pub_interval_1975_1984", "pub_interval_1965_1974",
                   "pub_interval_1900_1964", "pub_interval_1824_1899")


# Define K values for each interval
K_values <- c(44, 44, 44, 45, 45, 44, 45, 43, 40, 16, 2)

# Loop over each publication interval
for (idx in seq_along(pub_intervals)) {
  pub_interval <- pub_intervals[idx]
  print(paste("Processing interval:", pub_interval))
  
  # Filter data for the current publication interval
  data_filtered <- data[data[[pub_interval]] == 1, ]

# Save the original title data for future use
data_filtered$original_concatenated_title_abstract <- data_filtered$concatenated_title_abstract

#pre-processing the titles using textProcessor from the stm package
processed_text <- textProcessor(data_filtered$concatenated_title_abstract, metadata = data_filtered) 

# Further prepare the data by removing low-frequency terms
out_text <- prepDocuments(processed_text$documents, processed_text$vocab, processed_text$meta)
docs_text <- out_text$documents
vocab_text <- out_text$vocab
meta_text <- out_text$meta


#Prepare data
plotRemoved(processed_text$documents, lower.thresh = seq(1, 200, by = 100))
out_text <- prepDocuments(processed_text$documents, processed_text$vocab, processed_text$meta, lower.thresh = 15)


# Initialize an empty formula string
prevalence_formula_str <- "~US "


# Add each country variable to the formula string
for (i in 10:27) {
  prevalence_formula_str <- paste(prevalence_formula_str, "+", names(data)[i])
}


# Convert the string to a formula
prevalence_formula <- as.formula(prevalence_formula_str)
print(prevalence_formula)

# Run STM model
Research_topics <- stm(documents = out_text$documents, 
                             vocab = out_text$vocab, 
                             K = K_values[idx], 
                             prevalence = prevalence_formula, 
                             data = out_text$meta, 
                             init.type = "Spectral",
                             max.em.its = 1000,
                             gamma.prior = 'L1')


# Plot the STM model summary
plot(Research_topics, type = "summary", xlim = c(0, 0.3))

# Print the top 10 labels for each topic
topic_labels <- labelTopics(Research_topics, n=10)
print(topic_labels)

# Match the processed documents with the original titles
matched_titles <- out_text$meta$original_concatenated_title_abstract

# Print top 5 documents for each topic
top_docs <- findThoughts(Research_topics, texts = matched_titles, n = 5)$docs[[1]]
print(top_docs)

# Find and plot the key "thoughts" or documents for selected topics
thoughts6 <- findThoughts(Research_topics, texts = matched_titles, n = 3, topics = 6)$docs[[1]]
thoughts18 <- findThoughts(Research_topics, texts = matched_titles, n = 3, topics = 18)$docs[[1]]
par(mfrow = c(1, 2), mar = c(0.5, 0.5, 1, 0.5))
plotQuote(thoughts6, width = 30, main = "Topic 6")
plotQuote(thoughts18, width = 30, main = "Topic 18")

# Calculate and plot the correlation between topics
mod.out.corr <- topicCorr(Research_topics)
plot(mod.out.corr, cex = 1.5)


# For each topic
for (topic_num in 1:K_values[idx]) {
  # Plot the word cloud
  cloud(Research_topics, topic = topic_num, scale = c(2, 0.25))
  Sys.sleep(2)
}

# Get the topic proportions for each document
topic_proportions <- Research_topics$theta

# Find the index of the topic with the highest proportion for each document
# This will be the topic that each document is most likely to belong to
max_topic_idx <- apply(topic_proportions, 1, which.max)

# Add this as a new column to your data
data_filtered$topic <- max_topic_idx


# Define the countries
countries <- c("US", "GB", "DE", "IT", "AU", "CA", "JP", "CN", "NL", "FR", "RU", "ES", "IN", "IL", "CH", "Americas", "Europe", "Africa", "AsiaAndOceania")



# Create function to generate histogram
make_histogram <- function(country_code) {
  
  # Filter data for country with contribution of 100
  data_100 <- data_filtered[data_filtered[[country_code]] == 100,]
  
  # Filter data for country with contribution not equal to 0
  data_not_0 <- data_filtered[data_filtered[[country_code]] != 0,]
  
  # Make the first histogram
  hist_100 <- ggplot(data_100, aes(x = topic)) +
    geom_histogram(binwidth = 1, fill = "blue", color = "black") +
    xlab("Topic") +
    ylab("Count") +
    ggtitle(paste(country_code, "Independent Contribution"))
  
  # Make the second histogram
  hist_not_0 <- ggplot(data_not_0, aes(x = topic)) +
    geom_histogram(binwidth = 1, fill = "blue", color = "black") +
    xlab("Topic") +
    ylab("Count") +
    ggtitle(paste(country_code, "Contributions (with collaboration)"))
  
  # Return list of plots
  return(list(hist_100, hist_not_0))
}

# Loop over country codes and generate plots
for (country in countries) {
  plots <- make_histogram(country)
  
  # Print the plots
  print(plots[[1]])
  print(plots[[2]])
}
}


```

